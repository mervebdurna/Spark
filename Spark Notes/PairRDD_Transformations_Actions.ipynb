{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairRDD Transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Genelde aggragation islemleri icin kullanilir.\n",
    "* RDD'nin cift(pair) haline getirilmesi icin on hazirlik gerektirebilir. ('Ali','Ahmet','Cem) --> ((Ali,1),(Ahmet,1),(Cem,1))\n",
    "* Bu yapi uzerinde spark daha ozel operasyonlara imkan verir.\n",
    "* Temel veri yapisi tuple dir\n",
    "* ornek operasyonlar : \n",
    "\n",
    "    En cok yorum yapan kim ? \n",
    "    \n",
    "    En fazla puan alan film hangisi ?\n",
    "    \n",
    "    Mesleklere gore ortalama kazanc nedir ? \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tek Bir RDD icin PairRDD Transformations\n",
    "* reduceByKey()  : Anahtarlar icin degerleri birlestirir.\n",
    "* groupByKey() : Anahtarlara gore gruplar. \n",
    "* combineByKey()\n",
    "* mapValues() : x degerleri valuelar uzerinde gezer. \n",
    "* keys() : PairRdd nin anahtarlarini iceren yeni bir rdd doner.\n",
    "* values() :PairRdd nin degerlerini  iceren yeni bir rdd doner.\n",
    "* sortByKey() : Anahtara gore siralar (en cok an az tekrarlanan degerleri bulmak icin kullanilir)\n",
    "\n",
    "Not:Tupple larda ilk deger key oikinci deger value olarak isimlendirilir.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = SparkConf().\\\n",
    "setMaster('local[2]').\\\n",
    "setAppName(\"Test\").\\\n",
    "set(\"spark.driver.mermory\",\"2g\").\\\n",
    "setExecutorEnv(\"spark.executer.memory\",\"3g\")\n",
    "\n",
    "sc = SparkContext(conf=spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 5), (3, 4), (2, 1), (2, 6), (4, 7)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(3,5),(3,4),(2,1),(2,6),(4,7)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 6), (4, 7), (3, 20)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey()\n",
    "rdd.reduceByKey(lambda x,y : x*y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, '16'), (4, 7), (3, '54')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y : str(x)+str(y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, <pyspark.resultiterable.ResultIterable at 0x21aff31d348>),\n",
       " (4, <pyspark.resultiterable.ResultIterable at 0x21aff31d248>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x21aff31d588>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#groupByKey\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combineByKey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 105), (3, 104), (2, 101), (2, 106), (4, 107)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues\n",
    "rdd.mapValues(lambda x : x+100).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 2, 2, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys()\n",
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 1, 6, 7]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values()\n",
    "rdd.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (2, 6), (3, 5), (3, 4), (4, 7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey\n",
    "rdd.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iki  RDD icin PairRDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subtractByKey\n",
    "\n",
    "* join()\n",
    "\n",
    "* rightOuterJoin()\n",
    "\n",
    "* leftOuterJion()\n",
    "\n",
    "* cogroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(3,5),(1,4),(2,1),(6,6),(4,7)])\n",
    "rdd2 = sc.parallelize([(3,5),(2,5),(3,7),(2,1),(7,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 7), (1, 4), (6, 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractByKey\n",
    "rdd.subtractByKey(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (1, 5)), (2, (1, 1)), (3, (5, 5)), (3, (5, 7))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join() : inner join\n",
    "rdd.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, (1, 5)), (2, (1, 1)), (3, (5, 5)), (3, (5, 7)), (7, (None, 1))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rightOuterJoin()\n",
    "rdd.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, (7, None)),\n",
       " (1, (4, None)),\n",
       " (2, (1, 5)),\n",
       " (2, (1, 1)),\n",
       " (6, (6, None)),\n",
       " (3, (5, 5)),\n",
       " (3, (5, 7))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leftOuterJoin()\n",
    "rdd.leftOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316f08>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316188>)),\n",
       " (1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316348>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316288>)),\n",
       " (2,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316108>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316ec8>)),\n",
       " (6,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316848>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316dc8>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316c48>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316e48>)),\n",
       " (7,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x21a81316bc8>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x21a81316d88>))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cogroup() : tum kombinasyonlari getirir. \n",
    "rdd.cogroup(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
